I"!<div style="text-align: center;">
    <img src="/assets/img/InfoGAIL/abstract.png" width="90%" />
</div>

<h2 id="abstract">Abstract</h2>

<p>모방학습(Imitation Learning)의 목표는 명시적인 보상 신호에 대한 접근 없이도 전문자의 행동을 모방할 수 있는 것이다. 인간 전문가의 시연은 잠재적인 요인으로 인해 명확한 차이들을 보이며 이는 보통 명시적으로 모델링이 되어있지 않다. 본 논문에서는 전문가 시연의 잠재적인 구조를 비지도 학습 방법을 통해 추론할 수 있는 새로운 알고리즘을 소개한다. GAIL(Generative Adversarial Imitation Learning)을 근간으로 하는 이 방법은 복잡한 행동을 모방할 수 있을뿐만 아니라, 시각적 시연을 포함한 복잡한 행동 데이터에 대한 해석적이고도 의미있는 표현(Representation)을 학습할 수 있게 해준다. 주행 도메인에서는 사람의 시연으로부터 학습된 모델이 다양한 행동을 생성하고 raw visual input을 이용해 사람의 행동을 정확하게 예측할 수 있음을 보여주었다. 다양한 baseline과 비교해볼 때, 본 방법은 전문가 시연 아래 잠재적인 구조를 더욱 잘 파악할 수 있으며 대부분의 경우 데이터마다의 차이로부터 비롯되는 의미있는 요인을 복구할 수 있다.</p>

<blockquote>
  <p>당시 Imitation Learning(IL) 연구의 흐름을 토대로 볼때, GAIL에 다른 요소를 추가하여 새로운 알고리즘인 InfoGAIL을 고안한 것으로 보임. 전문가 시연 데이터마다 전문가들의 성격?, 스타일? 등이 다를텐데 그 속에 내제된 잠재 구조에 대해서 파악하고, 복잡한 task(자율주행)에 대해서 오직 raw image input을 받아 학습할 수 있다는 장점이 있는 것으로 보임.</p>
</blockquote>

<h2 id="1-introduction">1. Introduction</h2>

<p>Reinforcement Learning(RL)의 가장 큰 한계점은 사전에 정의된 보상 함수 혹은 보상 신호를 최적화하는 과정을 포함하는 것이다. 체스나 바둑과 같은 경우는 보상 함수를 명시적으로 정의하는게 적합할 수 있다. 하지만, 이보다 복잡하고, 환경에 대한 구체화가 잘 되어있지 않은 (자율 주행 환경은 안전, 편안함 그리고 효율성이 균형을 이뤄야 함) 경우에는 적합한 보상함수의 설계가 어려워질 수 있다.</p>

<p>모방학습 방법은 전문가의 시연으로부터 직접적으로 학습함으로써 이러한 문제를 완화할 수 있는 가능성을 제시하며, 넓은 범위의 문제들에서 성공적인 결과를 보이고 있다. 그것들 중, GAIL은 model-free인 Imitation Learning 방법으로 굉장히 효과적이며 상대적으로 높은 차원의 환경에까지 scaling이 잘 될 수 있다. GAIL을 학습시키는 과정은 고정된 시뮬레이션 환경에서 전문가의 시연과 비슷한 행동을 생성해내는 생성 모델 즉, stochastic policy를 만드는 것으로도 생각될 수 있다. 이러한 유사성은 GAN에서와 같은 방식으로 Discriminator(구별자)를 통해 expert trajectory들과의 구별을 jointly하게 학습시키는 과정으로 얻어진다.</p>

<p>모방학습에서는 주로 사람에게서 얻어진 시연 데이터를 사용하는데, 이러한 시연들간에는 명확한 차이가 존재한다. 가령 이러한 시연들은 여러 전문가들로부터 수집되었을 수 있으며, 각각의 전문가는 서로 다른 policy를 가졌을 것이다. 시뮬레이션 환경이 감지하지 못하는 이러한 차이들에 대한 외부 잠재 요인또한 관찰된 행동들에 명확한 영향을 줄 수 있다 (이 부분은 솔직히 무슨 말인지 모르겠다). 가령 전문가 시연은 서로 다른 스킬과 습관을 가진 사용자로부터 수집되었을 수 있다. 본 논문의 목표는 전문가의 시연 아래 내제된 차이들에 대한 잠재 요인을 자동적으로 찾아 분해할 수 있는 모방학습 프레임워크를 개발하는 것이다. Reference [14]에서 이미지 생성 모델로부터 얻어진 style, shape 그리고 color를 분해했던 것처럼 여기서도 비지도 학습 방식으로 사람의 시연으로부터 유사한 해석가능한 컨셉을 자동적으로 학습해보고자 하는 것이다.</p>

<p>본 논문에서는 동적인 환경의 trajectory를 생성할 수 있는 잠재 변수 생성 모델을 학습할 수 있는 새로운 방법을 소개한다 (MDP 상의 상태, 행동의 pair).</p>

<h2 id="2-background">2. Background</h2>

<h2 id="3-interpretable-imitation-learning-through-visual-input">3. Interpretable Imitation Learning through Visual Input</h2>

<p>시연은 주로 전문가인 사람으로부터 수집된다. 결과적인 trajectory들은 명확한 차이들을 보일 수 있는데, 이는 개개인의 숙련도의 차이나 선호하는 전략의 차이와 같이 이들의 내부적 잠재 요인으인으로부터 비롯될 수 있다. 심지어 같은 개인이더라도 같은 상황에서 다른 선택을 할 수도 있으며 이는 잠재적으로 near-optimal한 여러개의 “다른” policy들로부터 시연을 만들어낸다.</p>

<p>형식적으로, 전문가 policy는 여러 전문가 policy들이 혼합된 형태라고 가정한다. 여기서의 목표는 잠재 변수를 포함한 policy pi(a|s, c)를 pi_E의 근사적인 형태로 복구하는 것이다. 이때 c는 prior <em>p(c)</em>로부터 sampling된 것이고, conditional policy pi(a|s,c)로부터 생성된 trajectory tau는 전문가 trajectory tau_E와 비슷해야한다. 그리고 이 유사한 정도는 Discriminative 분류기를 통해 측정될 수 있다.</p>

<div style="text-align: center;">
    <img src="/assets/img/InfoGAIL/gail_pseudo_code.png" width="96%" />
</div>

<p>위는 GAIL의 Pseudo Code이다. 본 논문에서는 만약 pi(a|s,c)를 GAIL에 직접 적용을 해보면 c를 무시하고 전문가 시연에 존재하는 서로 다른 행동들의 분리가 불가할 것이라고 주장한다 (어떤 근거인지는 잘 모르겠음). 따라서, 모델이 c를 최대한 활용할 수 있도록 본 논문에서는 <strong>information-theoretic Regularization</strong> 기법을 부과한다. 그리고 이는 c와 trajectory의 상태-행동 pair들 사이에 강한 mutual information이 있다고 생각하는 것이다. 이러한 개념은 InfoGAN [14]에서 소개를 하였으며, latent code가 데이터 분포의 의미적인 특징을 찾고, 생성 과정을 지도하는데 사용되는 것에 착안한 것이다. 특히, <em>I(c;tau)</em>로 표기되는 정규화는 latent code들과 trajectory들 사이의 mutual information을 최대화하도록 한다. 하지만, 이를 직접적으로 최대화하는 것은 Posterior P(c|tau)에 직접 접근성이 필요하기 때문에 어렵다. 따라서, mutual information의 variational lower bound <em>L_I(pi, Q)</em>를 도입한다.</p>

<div style="text-align: center;">
    <img src="/assets/img/InfoGAIL/eq2.png" width="50%" />
</div>
<p><br /></p>

<p>이때, <em>Q(c|tau)</em>는 true Posterior의 근사이다. 이러한 정규화 아래 목적 함수는 <strong>Information Maximizing Generative Adversarial Imitation Learning (InfoGAIL)</strong>라고 불리며 다음과 같이 나타내어진다.</p>

<div style="text-align: center;">
    <img src="/assets/img/InfoGAIL/eq3.png" width="77%" />
</div>
<p><br /></p>

<p>이때, lambda_1 &gt; 0은 information maximization regularization 항의 hyperparameter이고, lambda_2 &gt; 0는 casual entropy 항에 대한 hyperparameter이다. 목적 함수의 최적화를 하는 데 있어, 전체 trajectory tau를 직접 사용하는 것은 computationally expensive할 수 있기 때문에 (Image와 같이 observation의 차원이 큰 경우), Posterior의 근사를 간단화한 <em>Q(c|s,a)</em>이 사용되었다. L_I(pi_theta, Q_psi)는 TRPO를 사용한 pi_theta와 stochastic gradient method를 통해 최적화가 진행되었다. 그리고 Q_psi는 Adam Optimizer를 사용해 업데이트 되었다.</p>

<div style="text-align: center;">
    <img src="/assets/img/InfoGAIL/infogail_pseudo_code.png" width="90%" />
</div>

<p><br /><br /></p>

<script type="text/javascript" src="https://cdnjs.buymeacoffee.com/1.0.0/button.prod.min.js" data-name="bmc-button" data-slug="ybkim95" data-color="#FFDD00" data-emoji="" data-font="Comic" data-text="Buy me a coffee" data-outline-color="#000000" data-font-color="#000000" data-coffee-color="#ffffff"></script>

:ET