I"*<h2 id="chapter-1-introduction"><strong>Chapter 1: Introduction</strong></h2>

<p>일반적으로 학습이라고 하면, 환경과 교류하면서 학습하는 것을 떠올릴 것입니다. 이 책에서는 그러한 환경과의 상호작용을 통해 배우는 것을 다루는데, 특히 수치화된 계산적 접근(computational approach)을 다룹니다. 즉, 사람과 동물이 배우는 것을 전체적으로 다루기보다는 수치적으로 단순화된 상황을 가정한 후, 거기서 벌어지는 학습에 대해 알아보고, 여러 가지 학습 방법들 각각의 효용 및 장단점을 알아보도록 합니다. 이 시리즈에서 다룰 강화학습(reinforcement learning)이라는 분야는 머신러닝(machine learning)의 한 분야이지만, 머신러닝 내의 다른 분야에 비해 훨씬 더 목표 지향적(goal-directed)입니다. 다시 말해 특정한 목표를 달성하는 것에 학습의 초점이 맞춰집니다.</p>

<p><strong>1.1 강화학습이란</strong></p>

<p>강화학습이란, 어떤 상황에서 어떤 행동을 해야 보상이 가장 높을지를 학습하는 것입니다. 강화학습을 다른 분야와 구분짓는 두 가지 중요한 특징은 1) 시행 착오를 거치며 상황을 검색해야 한다는 것과, 2) 보상이 즉각적이지 않을 수 있다는 것, 그리고 3) 종합적인 목표가 있다는 것입니다.</p>

<p>탐색(Exploration)과 이용(Exploitation) 간의 상충: 머신러닝에서 지도 학습(supervised learning)의 경우 데이터와 함께 참값(labeled data)이 주어지므로, 무조건 학습한 모델을 따른 최적의 선택만을 합니다. 강화학습이 이와 다른 이유는, 처음부터 모든 데이터에 따른 참값이 주어지지 않으므로 에이전트가 환경을 탐색해야 한다는 것입니다. 그래서 에이전트가 경험하고 학습한 모델이 제시하는 최적의 선택(Exploitation)을 할지, 아니면 최적이 아닌 선택을 해서 더 많은 데이터를 탐색(Exploration)하여 더 나은 모델을 학습해볼 가능성을 열 것인지의 두 선택지 사이에서 항상 고민해야 한다는 점입니다. 강화학습이 어려운 이유는 그 어떤 문제도, 한쪽에만 치중해서는 실패로 연결된다는 데 있습니다.
즉각적이지 않은 보상(delayed reward): 지도 학습의 경우 각 데이터 포인트별로 독립 변수(independent variable, x)와 종속 변수(dependent variable, y)가 매칭이 되고, X값에 따른 y값이 무엇인지 계산하는 모델을 학습합니다. 그래서 X값을 모델에 인풋하여 나온 y 추정치가 실제 y와 다르면 모델이 해당 X에 대해 틀렸다는 것을 즉각적으로 알 수 있습니다. 그러나 강화학습에서는 어떤 상황을 인풋으로 하여 모델이 특정한 행동을 아웃풋한다고 하더라도, 그 행동이 좋은 행동이었는지는 나중에 가야 알 수 있는 경우가 많습니다. 그리고 나중에 보상이 주어졌을 때, 과연 그 보상이 있기까지 행한 숱한 행동 중에 어느 것이 정말 보상으로 이끌었는지 판별해 내야하는 문제(credit assignment)가 있습니다. 예를 들어 체스의 경우, 게임이 끝나봐야지 이겼는지 졌는지를 알 수 있습니다.
종합적인 목표 설정: 다른 분야가 주로 특정한 하위 문제(subproblem)들을 풀어 전체를 해결하려하는 경향이 있다면, 강화학습은 반대로 보상(reward)이라는 구체적인 가이드를 통해 전체적인 목표를 염두에 두고 행동하는 에이전트를 학습시킵니다.</p>

<p>1.2 강화학습의 예</p>

<p>먼저, 챕터 3에서 더 자세하게 정의하겠지만, 용어에 대해 간단히 알아봅니다.</p>

<p>에이전트(agent): 행동하고 선택하는 주체를 말합니다. 체스의 경우 체스 플레이어, 로봇 문제의 경우 로봇이 에이전트입니다.
환경(environment): 에이전트와 상호 작용하는, 에이전트 외부의 모든 요소들이 환경입니다.
상태(state): 환경의 여러 가지 형태입니다. 체스 문제의 경우 체스판과 그 위 장기말들이 환경을 이루고, 장기말이 어떤 식으로 배열되어 있는지가 상태를 정의합니다. 체스판의 상태가 이런 배열이면 이런 행동을 하고, 저런 배열이면 저런 행동을 하도록 우리는 학습을 진행하는데, 여기서 이런 배열, 저런 배열 등이 모두 각기다른 상태입니다.
강화학습의 예로는 체스, 공장 제어 시스템, 사슴이 태어나서 달리는 것을 배우는 방법, 로봇, 사람이 아침 식사를 준비하는 행동 등이 있습니다. 이 모든 예에서 공통되는 점은 다음과 같습니다.</p>

<p>환경(environment)과 의사를 결정하는 에이전트(decision-making agent) 사이에 상호 작용이 있습니다.
모든 에이전트는 환경이 주는 불확실성(uncertainty) 내에서 어떤 목적(goal)을 달성하려 합니다.
에이전트의 행동(action)은 환경의 다음 상태(future state)에 영향을 주며, 그 결과는 정확히 예측할 수는 없습니다.
올바른 선택이란, 에이전트의 행동이 야기하는 차후의 결과(delayed consequences)까지 고려를 한 선택입니다.
모든 예에서, 에이전트는 본인의 경험을 통해 선택을 개선해 나갈 수 있습니다.
주) 실무에서 어떤 당면한 문제에 강화학습을 적용하려 할 때, 위의 사항들을 고려하는 것은 매우 중요합니다. 어떤 문제는 얼핏보면 강화학습이 잘 들어맞을 것 같지만, 실제로는 그렇지 않은 경우도 있습니다. 주식 시장에서 매매를 하는 것이 그 한 예입니다. 강화학습이 성립되려면 에이전트의 행동(action) 선택이 그 이후의 환경 상태(state)에 영향을 주어야 합니다. 그러나 주식 시장에서 개인이 주식을 어떤 식으로 사고 판들(action), 그것이 주식 시장의 상태(state)인 시장 가격에 주는 영향은 미미합니다. 그래서 이 분야에서 강화학습이 쓰이는 케이스는 매매 주체의 자본이 충분히 클 때 매우 단기적인 시장 상황을 토대로 모델링할 때입니다. 그렇지 않은 상황에서는 당면한 문제를 잘 재정의해야만 강화학습을 적용할 수 있고, 그게 아니라면 지도 학습을 적용하여야 합니다.</p>

<p>1-3. 강화학습의 요소</p>

<p>강화학습은 행동 정책(policy), 보상 시그널(reward signal), 가치 함수(value function), 그리고 환경의 모델(model of the environment)로 이루어집니다.</p>

<p>행동 정책(policy)은 관찰된 환경의 상태(state)를 그에 적절한 행동(action)으로 매핑하는 함수입니다. 즉, 어떤 상황에서는 어떤 행동을 하라는 가이드입니다. 정책은 단순 함수일수도 있고, 간단한 색인 함수(lookup function)일 수도 있으며, 혹은 각 행동(action)별 확률(probability)을 배정하는 확률적(stochastic) 정책일 수도 있습니다.
보상 시그널(reward signal)은 해당 강화학습 문제의 목적을 정의합니다. 높은 보상이 있는 쪽으로 에이전트는 행동 정책을 수정해 나가야 합니다. 이 보상 시그널 또한 확률적인 시그널일 수 있습니다. (예를 들어 이 문을 열면 40% 확률로 6의 보상이 있고, 60% 확률로 -3의 보상이 있는 식)
가치 함수(value function)는 해당 상태(state)가 장기적으로 갖는 효용입니다. 보상 시그널이 단기적인 효용이라면, 가치 함수는 해당 상태(state)에서 미래에 가질 수 있는 보상 시그널들까지 고려한, 조금 더 장기적인 효용을 본다고 생각하시면 됩니다. 챕터 3에서 자세히 설명합니다.
환경의 모델(model)은 실제 환경을 단순화하고 각 환경의 요소들이 어떻게 작용(dynamics)하는지를 정의한 것입니다. 챕터 3에서 나오겠지만, 이 책에서는 많은 문제들을 MDP(Markov Decision Process)라는 형태로 모델링합니다. 이러한 모델들을 이용해서 최적 행동 정책을 계산하는 알고리즘들을 model-based 방식들이라고 하고, 환경에 대해 어떠한 가정이나 모델링도 하지 않고 단순 시행착오를 거쳐 최적의 정책을 알아내는 알고리즘들을 model-free 방식들이라고 합니다.</p>

<p>1-4. 강화학습의 한계</p>

<p>강화학습은 환경의 상태(state)라는 개념에 많이 의존하므로 이를 잘 정의해야 합니다. 예를 들어 체스판에서는 상태(state)를 체스 말들의 위치 정보로 정의해야지, 체스판의 두께 등을 상태(state)에 포함시켜서는 문제를 제대로 풀 수 없습니다. 또한 문제에 따라, 센서 등에서 오는 상태 정보는 전처리(preprocessing)을 잘해야 합니다. 본 책에서는 그러한 부분들을 다루지 않지만 결코 그것이 중요하지 않다는 뜻은 아닙니다.
본 책에서 다루는 대다수의 알고리즘은 가치 함수(Value Function)를 찾는데 초점이 맞춰져 있지만, 반드시 가치 함수를 이용한 강화학습 방법만 있는 것은 아닙니다. 예를 들어 유전적 알고리즘(genetic algorithm)들은 여러 정책을 테스트해 가장 많은 보상을 받은 정책이 살아남는, 소위 말하는 진화론적 방법(evolutionary methods)을 사용합니다. 이 방식의 강화학습은 환경의 상태(state)를 완벽히 알 수 없을 때 장점을 가지지만, 효율면에서 좋다고 보기는 힘든 상황이 많습니다.</p>

<p>1-5. Tic-Tac-Toe 예제</p>

<p>책에서 Tic-Tac-Toe 게임을 통해 Value Iteration 등의 알고리즘을 간략히 설명하지만, 어차피 차후 챕터들에서 상세히 기초부터 다룰 내용이라 생략합니다. 저자가 설명하고 싶은 논지 중 주요한 부분으로는,</p>

<p>Tic-Tac-Toe의 보드는 행과 열이 3개씩일 뿐이므로 state space가 매우 작지만, 강화학습은 환경의 state space가 매우 크거나 심지어 무한할 때에도 적용 가능합니다.
State space가 클 경우에는 모든 state의 value를 직접 산정할 수 없으므로 신경망(neural network)와 같은 알고리즘을 통해 과거 경험을 일반화(generalize)해야 합니다. 물론 그것을 하는데 있어 항상 딥러닝이 가장 좋은 방법인 것은 아닙니다.
Tic-Tac-Toe 같은 경우 환경을 정의하고 묘사한 모델이 있었지만, 강화학습에 있어 그런 모델이 꼭 필요한 것은 아닙니다.
이후는 챕터 요약과 강화학습의 역사에 대해서 여러 장을 할애하는데 생략하도록 하겠습니다.</p>

<p><a href="https://www.aicrowd.com/challenges/neurips-2021-minerl-diamond-competition">Reference</a></p>
:ET