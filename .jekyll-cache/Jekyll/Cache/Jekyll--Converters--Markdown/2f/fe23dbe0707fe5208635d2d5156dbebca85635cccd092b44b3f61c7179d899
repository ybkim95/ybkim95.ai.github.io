I"ö:<h2 id="introduction"><strong>Introduction</strong></h2>

<p>í•´ë‹¹ í”„ë¡œì íŠ¸ëŠ” ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ ì¤‘ <strong>DDPG(Deep Deterministic Policy Gradient)</strong>ë¼ëŠ” ì•Œê³ ë¦¬ì¦˜ì„ ììœ¨ì£¼í–‰ ìë™ì°¨ taskì— ì ìš©í•˜ëŠ” ë‚´ìš©ìœ¼ë¡œ ì§„í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.
ë³¸ í”„ë¡œì íŠ¸ì—ì„œ ì •ì˜í•˜ëŠ” ë¬¸ì œëŠ”, ë¬¼ë¦¬ ì—”ì§„ (Bullet 2.78)ì´ ì¥ì°©ëœ simulator ìƒì—ì„œ ì°¨ëŸ‰ì˜ ëª¨ë¸ì´ ì£¼ì–´ì§„ë‹¤ê³  í•  ë•Œ, laneìœ¼ë¡œë¶€í„°ì˜ ì´íƒˆì—†ì´ íŠ¹ì • goalê¹Œì§€ ìµœëŒ€í•œ ë¹ ë¥´ê²Œ ì´ë™í•˜ëŠ” ê²ƒì´ì—ˆìŠµë‹ˆë‹¤.
ë³¸ í”„ë¡œì íŠ¸ì—ì„œëŠ” Policy Gradient ê¸°ë°˜ì˜ ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì¸ DDPG ì•Œê³ ë¦¬ì¦˜ì´ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤. Policy Gradient ê³„ì—´ì˜ ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì€ objective functionì˜ ë¯¸ë¶„ ê°’ì„ ë°”íƒ•ìœ¼ë¡œ gradient ascent ê¸°ë²•ì„ í™œìš©í•˜ì—¬ ê°€ì¥ í° return ê°’ì„ ì¤„ ìˆ˜ ìˆëŠ” policyë¥¼ ì°¾ëŠ” ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.
ê¸°ì¡´ <strong>DQN(Deep Q Network)</strong> ê¸°ë²•ì—ì„œëŠ” high-dimensionalí•œ inputì„ ì‚¬ìš©í•  ìˆ˜ëŠ” ìˆì—ˆì§€ë§Œ, discreteí•˜ê³  ì°¨ì›ì´ ë‚®ì€ action spaceë¥¼ ì§€ë‹Œ taskë§Œ í•´ê²° ê°€ëŠ¥í–ˆì—ˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ, DQNì—ì„œëŠ” ì‹œìŠ¤í…œ actionì˜ ì¢…ë¥˜ê°€ ëŠ˜ì–´ë‚ ìˆ˜ë¡ ì°¨ì›ì˜ ì €ì£¼ì— ê±¸ë¦°ë‹¤ëŠ” ë¬¸ì œì ì´ ìˆì—ˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê³ ì, Deep Neural Networkë¡œ ê·¼ì‚¬í™”í•œ action-value functionì„ ì‚¬ìš©í•˜ëŠ” Model-free, Off-policy Actor-Critic ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆí•˜ì˜€ê³ , ì´ ì•Œê³ ë¦¬ì¦˜ì˜ ì´ë¦„ì„ <strong>Deep Deterministic Policy Gradient(DDPG)</strong>ë¼ê³  ëª…ëª…í–ˆìŠµë‹ˆë‹¤.</p>

<h2 id="task"><strong>Task</strong></h2>

<iframe width="650" height="335" src="https://www.youtube.com/embed/uJyL-RYrtzs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe>
<p><br /></p>

<h2 id="deep-deterministic-policy-gradientddpg"><strong>Deep Deterministic Policy Gradient(DDPG)</strong></h2>

<ul>
  <li>DDPGëŠ” model-free off-policy actor-critic ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ Deep Q Learning(DQN)ê³¼ DPGì˜ ì»¨ì…‰ì„ ê²°í•©í•œ ì•Œê³ ë¦¬ì¦˜ì´ë‹¤. DQNì€ discreteí•œ action spaceì—ì„œ ì‘ë™ë˜ê³ , DPGëŠ” ì´ë¥¼ continuousí•œ action spaceë¡œ í™•ì¥í–ˆìœ¼ë©°, deterministicí•œ policyë¥¼ ì‚¬ìš©í•˜ì˜€ë‹¤.</li>
  <li>ì´ ì•Œê³ ë¦¬ì¦˜ì´ off-policyì¸ë§Œí¼, ì´ëŠ” actorì™€ criticì´ë¼ëŠ” networkë¥¼ ê°€ì§„ë‹¤. actorëŠ” exploreë¥¼ í•˜ê¸° ìœ„í•œ actionì„ ìƒì„±í•œë‹¤. actorì˜ update ê³¼ì •ë™ì•ˆ criticìœ¼ë¡œë¶€í„° ìƒê¸´ TD errorë¥¼ ì‚¬ìš©í•œë‹¤. critic networkëŠ” Q-learningì˜ update ruleê³¼ ë¹„ìŠ·í•œ TD errorë¥¼ ë°”íƒ•ìœ¼ë¡œ updateê°€ ìˆ˜í–‰ëœë‹¤.</li>
  <li>Q-learningì—ì„œ function approximatorë¡œ deep neural networkë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œì¨ instability issueê°€ ìƒê¸¸ ìˆ˜ ìˆëŠ” ê²ƒì„ í™•ì¸í•˜ì˜€ë‹¤. ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ experience replayì™€ target networksê°€ ì‚¬ìš©ë˜ì—ˆë‹¤.</li>
</ul>

<h2 id="on-policy-learning-vs-off-policy-learning"><strong>On-policy Learning vs Off-policy Learning</strong></h2>

<ul>
  <li>On-policyì˜ ê²½ìš°, ìì‹ ì´ ì§ì ‘ ì‹œí–‰ì°©ì˜¤ë¥¼ ê²ªìœ¼ë©´ì„œ ìŠ¤ìŠ¤ë¡œ ë°°ìš°ëŠ” ê²ƒì— ë¹„ìœ ë¥¼ í•  ìˆ˜ ìˆë‹¤. ë™ì¼í•œ policyì¸ \piì— ëŒ€í•˜ì—¬ samplingëœ ê²½í—˜ì„ ë”°ë¥´ë©´ì„œ ì´ë¥¼ í†µí•´ í•™ìŠµì„ í•˜ëŠ” ë°©ì‹ì„ ì˜ë¯¸í•œë‹¤.</li>
  <li>
    <p>ë°˜ë©´, Off-policyì˜ ê²½ìš° ë³¸ì¸ì´ ì•„ë‹Œ ë‹¤ë¥¸ì´ê°€ ì‹œí–‰ì°©ì˜¤ë¥¼ ê²ªëŠ” ê²ƒì„ ë³´ë©´ì„œ ë°°ìš°ëŠ” ê²ƒì— ë¹„ìœ ë¥¼ í•  ìˆ˜ ìˆë‹¤. ë‹¤ë¥¸ policyì¸ \muì— ëŒ€í•´ì„œ samplingëœ ê²½í—˜ì„ ë”°ë¥´ë©´ì„œ ìì‹ ì˜ policyì¸ \pië¥¼ í•™ìŠµí•˜ëŠ” ë°©ì‹ì„ ì˜ë¯¸í•œë‹¤.</p>
  </li>
  <li>
    <p>DDPGëŠ” high-dimensionalì´ë©´ì„œ continuousí•œ action spaceì˜ policyë¥¼ í•™ìŠµ ê°€ëŠ¥í•˜ë‹¤ëŠ” íŠ¹ì§•ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. DPGì™€ì˜ ë¹„êµë¥¼ í•˜ìë©´ DDPGëŠ” ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.</p>

    <p>DDPG = DPG + Actor-Critic with DNN</p>
  </li>
  <li>í•˜ì§€ë§Œ actor-criticì„ ê·¸ëŒ€ë¡œ ì ìš©í•˜ê¸°ì—ëŠ” í•™ìŠµì˜ convergeê°€ ì˜ ì•ˆëœë‹¤ëŠ” ë‹¨ì ì´ ìˆì—ˆê³ , ì´ë¥¼ í•´ê²°í•˜ê³ ì DQNì˜ ì•„ì´ë””ì–´ë¥¼ ë„ì…í•˜ì˜€ìŠµë‹ˆë‹¤.</li>
</ul>

<h2 id="actor-critic"><strong>Actor Critic</strong></h2>

<ul>
  <li>Actor-Criticì„ ê°„ë‹¨í•˜ê²Œ ì„¤ëª…í•˜ìë©´, ì´ëŠ” Policy Gradientì˜ Temporal Difference(TD) ë²„ì „ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ì´ëŠ” Actor networkì™€ Critic networkë¡œ êµ¬ì„±ëœë‹¤.</li>
  <li>ActorëŠ” ì–´ë– í•œ actionì´ ìˆ˜í–‰ë˜ì–´ì•¼ í•˜ëŠ”ì§€ë¥¼ ê²°ì •í•˜ê³ , Criticì˜ ê²½ìš° Actorì—ê²Œ í•´ë‹¹ actionì´ ì–¼ë§ˆë‚˜ ì¢‹ì•˜ê³ , ì–´ë–»ê²Œ ì¡°ì •ì„ í•´ì•¼í• ì§€ ì•Œë ¤ì£¼ê²Œ ëœë‹¤. Actorë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì€ policy gradient ì ‘ê·¼ ë°©ì‹ìœ¼ë¡œ ìˆ˜í–‰ë  ìˆ˜ ìˆìœ¼ë©°, Criticì€ Actorì— ì˜í•´ ìƒì„±ëœ actionì„ value functionì„ ê³„ì‚°í•¨ìœ¼ë¡œì¨ í‰ê°€í•œë‹¤.</li>
</ul>

<h2 id="reward-function"><strong>Reward Function</strong></h2>

<ul>
  <li>f(speed x): relative angle \thetaë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë³´ìƒí•¨ìˆ˜ëŠ” LiDAR ì„¼ì„œë¡œë¶€í„° ìˆ˜ì§‘ë˜ëŠ” ê±°ë¦¬ê°’ì˜ ì¡°í•©ê³¼ ì°¨ëŸ‰ì˜ real heading vectorì™€ ì°¨ëŸ‰ì˜ desired heading vector ì‚¬ì´ì˜ ê°ë„ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆë‹¤.</li>
  <li>
    <p>ì•„ë˜ ì½”ë“œì—ì„œì²˜ëŸ¼,</p>

    <p>1) f(LiDAR)ì˜ ê²½ìš°, FRONT / BACK / SIDE ì˜ì—­ì— í•´ë‹¹ë˜ëŠ” LiDAR rayë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì¼ì • ê±°ë¦¬ ë²”ìœ„ë¥¼ ì§€ì •í•˜ì—¬ threshold ê°’ì´ ì´í•˜ì´ë©´ ê·¸ì— ë§ëŠ” punishmentë¥¼ ì£¼ê³  ì¼ì • threshold ê°’ ì´ìƒì´ë©´ rewardë¥¼ ì£¼ëŠ” ë°©ì‹ìœ¼ë¡œ ì¶”ê°€í•´ì¤¬ë‹¤.</p>

    <p>2) f(speed_x)ì˜ ê²½ìš°, ìœ„ ê·¸ë¦¼ ìƒì—ì„œëŠ” ì˜ëª» í‘œê¸°ë˜ì–´ìˆì§€ë§Œ ì „ì§„í•´ì•¼í•˜ëŠ” ë°©í–¥ì„ ê¸°ì¤€ìœ¼ë¡œ ë‚˜ì•„ê°€ëŠ” ì†ë„ì˜ kë°°ë¥¼ í•´ì„œ rewardì— ì¶”ê°€í•´ì¤¬ë‹¤.</p>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">getReward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">reward_lidar</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># FRONT
</span>        <span class="k">if</span> <span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">lidar_front</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mf">0.25</span><span class="p">):</span>
            <span class="c1">#print(cl("[CLOSE] FRONT", "red"))
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">reward_lidar_front</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.</span> <span class="o">/</span> <span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">lidar_front</span><span class="p">)</span> <span class="o">/</span> <span class="mf">5.</span>
        <span class="k">elif</span> <span class="p">(</span><span class="mf">0.25</span> <span class="o">&lt;</span> <span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">lidar_front</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">0.5</span><span class="p">):</span>
            <span class="c1">#print(cl("[CLOSE] FRONT", "red"))
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">reward_lidar_front</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.</span> <span class="o">/</span> <span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">lidar_front</span><span class="p">)</span> <span class="o">/</span> <span class="mf">7.5</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1">#print(cl("[ENOUGH] FRONT", "green"))
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">reward_lidar_front</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">*</span> <span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">lidar_front</span><span class="p">)</span> <span class="o">/</span> <span class="mf">5.</span>
        
        <span class="c1"># BACK
</span>        <span class="k">if</span> <span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">lidar_back</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mf">0.25</span><span class="p">):</span>
            <span class="c1">#print(cl("[CLOSE] BACK", "red"))
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">reward_lidar_back</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.</span> <span class="o">/</span> <span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">lidar_back</span><span class="p">)</span> <span class="o">/</span> <span class="mf">5.</span> 
        <span class="k">elif</span> <span class="p">(</span><span class="mf">0.25</span> <span class="o">&lt;</span> <span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">lidar_back</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">0.5</span><span class="p">):</span>
            <span class="c1">#print(cl("[CLOSE] BACK", "red"))
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">reward_lidar_back</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.</span> <span class="o">/</span> <span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">lidar_back</span><span class="p">)</span> <span class="o">/</span> <span class="mf">7.5</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1">#print(cl("[ENOUGH] BACK", "green"))
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">reward_lidar_back</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">*</span> <span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">lidar_back</span><span class="p">)</span> <span class="o">/</span> <span class="mf">5.</span>

        <span class="c1"># SIDE
</span>        <span class="k">if</span> <span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">lidar_side</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mf">0.15</span><span class="p">):</span>
            <span class="c1">#print(cl("[CLOSE] SIDE", "red"))
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">reward_lidar_side</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.</span> <span class="o">/</span> <span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">lidar_side</span><span class="p">)</span> <span class="o">/</span> <span class="mf">8.</span> 
        <span class="k">elif</span> <span class="p">(</span><span class="mf">0.15</span> <span class="o">&lt;</span> <span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">lidar_side</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mf">0.3</span><span class="p">):</span>
            <span class="c1">#print(cl("[CLOSE] SIDE", "red"))
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">reward_lidar_side</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.</span> <span class="o">/</span> <span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">lidar_side</span><span class="p">)</span> <span class="o">/</span> <span class="mf">12.</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1">#print(cl("[ENOUGH] SIDE", "green"))
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">reward_lidar_side</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">*</span> <span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">lidar_side</span><span class="p">)</span> <span class="o">/</span> <span class="mf">8.</span>


        <span class="bp">self</span><span class="p">.</span><span class="n">reward_lidar</span> <span class="o">=</span>  <span class="nb">sum</span><span class="p">([</span><span class="bp">self</span><span class="p">.</span><span class="n">reward_lidar_front</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">reward_lidar_side</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">reward_lidar_back</span><span class="p">])</span>  
        <span class="bp">self</span><span class="p">.</span><span class="n">reward_vel</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">vx</span><span class="p">)</span><span class="o">*</span><span class="mf">3.</span>
       
        <span class="bp">self</span><span class="p">.</span><span class="n">reward</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">reward_lidar</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">reward_vel</span>
       
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">reward</span>
</code></pre></div></div>

<h2 id="result"><strong>Result</strong></h2>

<p><img src="/assets/img/ddpg_result.png" width="100%" height="60%" alt="Markdown Monster icon" style="float: center;" /></p>

<ul>
  <li>ë³¸ í”„ë¡œì íŠ¸ì—ì„œëŠ” ììœ¨ì£¼í–‰ ìë™ì°¨ê°€ lane ìƒì—ì„œ ì¥ì• ë¬¼ê³¼ì˜ ì¶©ëŒì—†ì´ ì£¼í–‰ì„ í•˜ëŠ” continuousí•œ action spaceì—ì„œ DDPG ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©í•´ë³´ëŠ” ê²ƒì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.</li>
  <li>í›ˆë ¨ì¤‘ ëŒ€ëµ 200 episode ê·¼ì²˜ì—ì„œ Rewardì˜ convergenceê°€ ê´€ì°°ë˜ì—ˆìœ¼ë©° ì•½ 100,000 stepsì— ê±¸ì³ training ë˜ì—ˆìŠµë‹ˆë‹¤.</li>
  <li>í…ŒìŠ¤íŠ¸ì¤‘ í›ˆë ¨ê³¼ ê°™ì€ stop conditionì„ ê°€ì§€ê³  20 episodeì— ëŒ€í•´ í‰ê°€ë¥¼ ì§„í–‰í•œ ê²°ê³¼, ì—¬ëŸ¬ ì¢…ë¥˜ì˜ laneì— ëŒ€í•´ì„œ collision ì—†ì´ ê³„ì†ì ìœ¼ë¡œ ì£¼í–‰ ê°€ëŠ¥í•œ ê²ƒì„ ê´€ì°°í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.</li>
</ul>

<h2 id="paper"><strong>Paper</strong></h2>

<p><img src="/assets/img/ddpg1.png" width="85%" height="60%" alt="Markdown Monster icon" style="float: center;" /></p>

<p><img src="/assets/img/ddpg2.png" width="85%" height="60%" alt="Markdown Monster icon" style="float: center;" /></p>

<p><br /></p>

<p>For more information please refer to this <a href="https://drive.google.com/file/d/1h8Pxa18lu2fGG2SZx3r15dilqar-Izu7/view?usp=sharing">LINK</a></p>

:ET